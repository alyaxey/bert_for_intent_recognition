{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# download SNIPS and ATIS datesets for intent recognition\n\nimport requests\nimport pandas as pd\n\n\ndata_url = 'https://raw.githubusercontent.com/ZephyrChenzf/SF-ID-Network-For-NLU/master/data'\ndfs = []\nfor corpus in ['atis', 'snips']:\n    for part in ['train', 'valid', 'test']:\n        df = {}\n        for k, n in [('text', 'seq.in'), ('intent', 'label')]:\n            df[k] = requests.get('/'.join([data_url, corpus, part, n])).text.split('\\n')[:-1]\n        df = pd.DataFrame(df)\n        df['part'] = part\n        df['corpus'] = corpus\n        dfs.append(df)\ndf = pd.concat(dfs).reset_index(drop=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print maximum number of words\n\ndf.text.str.split(' ').str.len().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_id(tokenizer, max_length, str1, str2=None, truncation_strategy='longest_first'):\n    \"\"\"Tokenize and create inputs for transformers like BERT, RoBERTa and ALBERT\"\"\"\n\n    inputs = tokenizer.encode_plus(str1, str2,\n        add_special_tokens=True,\n        pad_to_max_length=True,\n        max_length=max_length,\n        truncation_strategy=truncation_strategy)\n\n    return [inputs[k] for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] if k in inputs]\n\ndef compute_input_arrays(df, tokenizer, max_length):\n    \"\"\"Transform dataset to transformer input\"\"\"\n    return [np.asarray(x, dtype=np.int32) for x in\n            zip(*(return_id(tokenizer, max_length, t) for t in tqdm(df.text)))]\n\ndef compute_output_arrays(df, intents_dict):\n    \"\"\"Transform dataset to target variable\"\"\"\n    return np.asarray(df.intent.apply(lambda i: intents_dict.get(i, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom transformers import AutoConfig, TFAutoModel, AutoTokenizer\n\n\nclass PositiveDense(tf.keras.layers.Layer):\n    \"\"\"Keras layer for linear combination of multiple layer outputs (last dimension)\n    with positive multipliers\"\"\"\n    def __init__(self):\n        super(PositiveDense, self).__init__()\n\n    def my_init(self, shape, dtype=None):\n        x = np.zeros(shape)\n        x[-1] = -3\n        return K.variable(value=x, dtype=dtype)\n        \n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim),\n                                      initializer=self.my_init,\n                                      name='kernel')\n        self.built = True\n\n    def call(self, inputs):\n        return K.sum(inputs * K.softmax(self.kernel), axis=-1)\n\n\nclass Print(tf.keras.layers.Layer):\n    \"\"\"Keras layer for debug printing of any variable\"\"\"\n    def __init__(self):\n        super(Print, self).__init__()\n        \n    def call(self, inputs, *args, **kwargs):\n        tf.print(\"var: \", inputs)\n        return inputs\n\n\ndef create_model(n_inputs, n_outputs, max_length):\n    \"\"\"BERT (or other transformer) model with:\n    - positive linear combination of all layer outputs\n    - dense layer on top\n    - multisample dropout https://arxiv.org/abs/1905.09788\"\"\"\n    x_in = [tf.keras.layers.Input((max_length,), dtype=tf.int32) for _ in range(n_inputs)]\n    \n    config = AutoConfig.from_pretrained(model_name, output_attention=False, output_hidden_states=True)\n    \n    bert_model = TFAutoModel.from_pretrained(\n        model_name, config=config)\n    \n    # (layers + 1) x batch_size x sequence x hidden_state\n    hidden_layers = bert_model(x_in)[2]\n\n    dropout = tf.keras.layers.Dropout(0.2)\n    \n    # batch_size x hidden_state x (layers + 1)\n    cls_outputs = K.stack(\n        [dropout(layer[:, 0, :]) for layer in hidden_layers],\n        axis=2)\n\n    # batch_size x hidden_state\n    cls_output = PositiveDense()(cls_outputs)\n\n    high_dropout = tf.keras.layers.Dropout(0.5)\n\n    top_classifier = tf.keras.layers.Dense(n_outputs, activation='sigmoid')\n    \n    # multisample dropout https://arxiv.org/abs/1905.09788\n    # batch_size x n_outputs\n    logits = K.mean(K.stack([\n        top_classifier(high_dropout(cls_output))\n        for _ in range(5)\n    ]), axis=0)\n    \n    model = tf.keras.models.Model(inputs=x_in, outputs=logits)\n    \n    return model\n\n\ndef train_test(model_name, batch_size, max_length, learning_rate, epochs, patience, data):\n    \"\"\"training and testing model on dataset, using early stopping with best model by validation accuracy\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    intents = np.concatenate([['_unk'], np.sort(data.intent[data.part == 'train'].unique())])\n    intents_dict = {intent: ind for ind, intent in enumerate(intents)}\n\n    outputs = compute_output_arrays(data, intents_dict)\n    inputs = compute_input_arrays(data, tokenizer, max_length)\n\n    train_inputs = [inp[data.part == 'train'] for inp in inputs]\n    train_outputs = outputs[data.part == 'train']\n\n    valid_inputs = [inp[data.part == 'valid'] for inp in inputs]\n    valid_outputs = outputs[data.part == 'valid']\n\n    K.clear_session()\n    model = create_model(len(train_inputs), len(intents_dict), max_length)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.fit(train_inputs, train_outputs, epochs=epochs, batch_size=batch_size, callbacks=[\n        tf.keras.callbacks.ModelCheckpoint(\n            'bert', monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=1),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy', patience=patience, restore_best_weights=True, verbose=1)\n    ], validation_data=(valid_inputs, valid_outputs))\n    preds = model.predict(inputs)\n\n    for part in ['train', 'valid', 'test']:\n        acc = accuracy_score(y_true=outputs[data.part == part],\n                             y_pred=np.argmax(preds[data.part == part], axis=-1))\n        print(f'{part:7} acc = {acc:.2%}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run model on ATIS and SNIPS datasets\n\nmodel_name = 'bert-base-uncased'\nbatch_size = 8\nmax_length = 64\nlearning_rate = 1e-5\nepochs = 20\npatience = 3\n\nfor dataset in ['atis', 'snips']:\n    print(dataset)\n    train_test(model_name=model_name,\n               batch_size=batch_size,\n               max_length=max_length,\n               learning_rate=learning_rate,\n               epochs=epochs,\n               patience=patience,\n               data=df[df.corpus == dataset])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}